/nfshomes/byan1/venvs/cmsc799/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Map:   0%|          | 0/56 [00:00<?, ? examples/s]Map: 100%|██████████| 56/56 [00:00<00:00, 1781.26 examples/s]
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
`torch_dtype` is deprecated! Use `dtype` instead!
You are using a model of type llava_next to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:03<00:06,  3.33s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:06<00:03,  3.13s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:08<00:00,  2.52s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:08<00:00,  2.70s/it]
Some parameters are on the meta device because they were offloaded to the cpu.
slurmstepd: error: *** JOB 5669746 ON cml00 CANCELLED AT 2025-10-31T06:26:33 DUE TO TIME LIMIT ***
